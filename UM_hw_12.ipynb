{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChecinskiBartlomiej/ML/blob/main/UM_hw_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework Assignment: Is a *queen* really just a *king*, minus a *man*, plus a *woman*?**\n",
        "\n",
        "--------------\n",
        "\n",
        "\n",
        "\n",
        "In class, we dealt with **embeddings** trained for **sentiment classification**. These embeddings are optimized to separate *positive* from *negative* expressions and **do not encode deeper semantic information**.\n",
        "\n",
        "However, in modern natural language processing, there exist other embeddings — such as those from **BERT**, **word2vec**, or **GloVe** — that **do capture semantic structure**. These models are trained on large corpora, and their embeddings often allow for meaningful **vector arithmetic**, like the famous:\n",
        "\n",
        "```\n",
        "embedding(\"king\") - embedding(\"man\") + embedding(\"woman\") ≈ embedding(\"queen\")\n",
        "```\n",
        "\n",
        "This homework explores **semantic vector relationships** using such pretrained embeddings.\n",
        "\n",
        "## **The Objective**\n",
        "\n",
        "Your task is to:\n",
        "\n",
        "1. Construct semantic classes of word pairs.\n",
        "2. Visualize them using PCA.\n",
        "3. Explore arithmetic operations in embedding space.\n",
        "\n",
        "## **Tasks & Deliverables**\n",
        "\n",
        "### 1. **Semantic Pair Classes**\n",
        "\n",
        "- You must gather **at least 10 classes** of semantically related word pairs.\n",
        "- Each class must contain **at least 5 pairs**.\n",
        "- That gives a **minimum total of 100 unique words** (10 classes x 5 pairs x 2 words per pair).\n",
        "\n",
        "Two example classes:\n",
        "\n",
        "**Class 1: Gender**\n",
        "\n",
        "- (king, queen)\n",
        "- (man, woman)\n",
        "- (doctor, nurse)\n",
        "- (prince, princess)\n",
        "- *(you must add one more)*\n",
        "\n",
        "**Class 2: Verb tense (past tense)**\n",
        "\n",
        "- (bring, brought)\n",
        "- (get, got)\n",
        "- (like, liked)\n",
        "- *(you must add two more)*\n",
        "\n",
        "**Your job:**\n",
        "\n",
        "- Invent or search for **at least 10 such classes**, including the examples above.\n",
        "- Each class must be conceptually coherent.\n",
        "- Other examples: singular/plural, country/capital, comparative/superlative, tool/user, job/object, etc.\n",
        "\n",
        "### 2. **Global PCA (Across All Words)**\n",
        "\n",
        "- Use PCA to reduce the **entire set of 100 word embeddings** to 2D, and plot it.\n",
        "- Plot the additional **10 separate charts**, one for each class.\n",
        "  - Each chart should display only the 10 words (5 pairs) of the given class.\n",
        "- Points should be labeled with the words themselves.\n",
        "\n",
        "### 3. **Local PCA (Per Class)**\n",
        "\n",
        "- For each class (10 total), perform PCA **only** on the 10 words of that class.\n",
        "- Plot these class-wise PCA visualizations as separate charts.\n",
        "- Again, points should be labeled with the words.\n",
        "\n",
        "**Total: 21 charts**\n",
        "(1 global plot with 100 words + 10 global-space class plots + 10 local PCA class plots)\n",
        "\n",
        "Charts should be presented in a self-explanatory manner with clear labels.\n",
        "\n",
        "### 4. **Embedding Arithmetic**\n",
        "\n",
        "For each class, choose **one example pair** (e.g., (king, queen)) and perform the operation:\n",
        "\n",
        "```\n",
        "embedding(B) - embedding(A) + embedding(C)\n",
        "```\n",
        "\n",
        "Where A and B form a known pair, and C is another base word.\n",
        "For example:\n",
        "\n",
        "```\n",
        "embedding(\"king\") - embedding(\"man\") + embedding(\"woman\")\n",
        "```\n",
        "\n",
        "* For each such result vector, find the **5 closest word embeddings** (using cosine similarity or Euclidean distance).\n",
        "* Print the top 5 neighbors **with their distances**.\n",
        "* Do this **once per class** (i.e., 10 times).\n",
        "\n",
        "This will make it possible to verify if\n",
        " ```\n",
        "embedding(\"queen\") ≈ embedding(\"king\") - embedding(\"man\") + embedding(\"woman\")\n",
        "```\n",
        "for the *gender*-related class.\n",
        "\n",
        "\n",
        "### 5. **Discussion**\n",
        "\n",
        "* Analyze and interpret your 21 plots.\n",
        "* Discuss whether the vector relationships are preserved.\n",
        "* Does PCA capture semantic differences?\n",
        "* Are the closest words from the arithmetic meaningful?\n",
        "* What kinds of relationships are captured, and what are not?\n",
        "* Are some classes better behaved than others?\n",
        "\n",
        "\n",
        "### 6. **Publish on GitHub**  \n",
        "   - Place the Colab notebook in your **GitHub repository** for this course.\n",
        "   - In your repository’s **README**, add a **link** to the notebook and also include an **“Open in Colab”** badge at the top of the notebook so it can be launched directly from GitHub.\n",
        "\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "*This homework assignment was inspired by an idea from my master's student **Andrzej Małek**, to whom I would like to express my thanks.*\n",
        "\n"
      ],
      "metadata": {
        "id": "_pm7h5fXtWfY"
      },
      "id": "_pm7h5fXtWfY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this project I will use BERT language model for embeddings. Based on transformer architecture BERT is capable of capturing semantic structure of text using $768$ dimensional vectors to represent tokens."
      ],
      "metadata": {
        "id": "stMmwYkjQTi0"
      },
      "id": "stMmwYkjQTi0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Bert Tokenizer and Bert Model.\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "BZo1gxEeQtdT"
      },
      "id": "BZo1gxEeQtdT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT has $30522$ tokens in its vocabulary. Because not every word is in vocabulary (it might be composed as some of the tokens from vocabulary), to have one vector representation for word I will average coordinates of tokens which build this word."
      ],
      "metadata": {
        "id": "obwC1lMSSHBg"
      },
      "id": "obwC1lMSSHBg"
    },
    {
      "cell_type": "code",
      "source": [
        "# Function which outputs mean of embeddings of word's tokens.\n",
        "import torch\n",
        "def get_word_embedding(word):\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word))\n",
        "    embeddings = model.get_input_embeddings()\n",
        "    with torch.no_grad():\n",
        "        token_embeddings = embeddings(torch.tensor(token_ids))\n",
        "    return token_embeddings.mean(dim=0)"
      ],
      "metadata": {
        "id": "wAqnjZAjT6_H"
      },
      "id": "wAqnjZAjT6_H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I will manually build a dictionary of classess and pairs of words which belongs to these classes. Each class is conceptually coherent."
      ],
      "metadata": {
        "id": "G9roolodVwp_"
      },
      "id": "G9roolodVwp_"
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_classes = {\n",
        "    \"gender\": [\n",
        "        (\"king\", \"queen\"),\n",
        "        (\"man\", \"woman\"),\n",
        "        (\"doctor\", \"nurse\"),\n",
        "        (\"prince\", \"princess\"),\n",
        "        (\"husband\", \"wife\")\n",
        "    ],\n",
        "    \"verb_tense_present_past\": [\n",
        "        (\"bring\", \"brought\"),\n",
        "        (\"get\", \"got\"),\n",
        "        (\"like\", \"liked\"),\n",
        "        (\"go\", \"went\"),\n",
        "        (\"see\", \"saw\")\n",
        "    ],\n",
        "    \"country_capital\": [\n",
        "        (\"France\", \"Paris\"),\n",
        "        (\"Japan\", \"Tokyo\"),\n",
        "        (\"Poland\", \"Warsaw\"),\n",
        "        (\"England\", \"London\"),\n",
        "        (\"Germany\", \"Berlin\")\n",
        "    ],\n",
        "    \"antonyms\": [\n",
        "        (\"hot\", \"cold\"),\n",
        "        (\"happy\", \"sad\"),\n",
        "        (\"light\", \"dark\"),\n",
        "        (\"young\", \"old\"),\n",
        "        (\"full\", \"empty\")\n",
        "    ],\n",
        "    \"country_currency\": [\n",
        "        (\"Mexico\", \"peso\"),\n",
        "        (\"India\", \"rupee\"),\n",
        "        (\"Spain\", \"euro\"),\n",
        "        (\"Hungary\", \"forint\"),\n",
        "        (\"Poland\", \"złoty\")\n",
        "    ],\n",
        "    \"gender_animal_male_female\": [\n",
        "        (\"lion\", \"lioness\"),\n",
        "        (\"bull\", \"cow\"),\n",
        "        (\"rooster\", \"hen\"),\n",
        "        (\"ram\", \"ewe\"),\n",
        "        (\"boar\", \"sow\")\n",
        "    ],\n",
        "    \"animal_meat\": [\n",
        "        (\"cow\", \"beef\"),\n",
        "        (\"pig\", \"pork\"),\n",
        "        (\"deer\", \"venison\"),\n",
        "        (\"sheep\", \"mutton\"),\n",
        "        (\"goat\", \"chevon\")\n",
        "    ],\n",
        "    \"musician_instrument\": [\n",
        "        (\"guitarist\", \"guitar\"),\n",
        "        (\"pianist\", \"piano\"),\n",
        "        (\"drummer\", \"drums\"),\n",
        "        (\"violinist\", \"violin\"),\n",
        "        (\"saxophonist\", \"saxophone\")\n",
        "    ],\n",
        "    \"us_state_capital\": [\n",
        "        (\"California\", \"Sacramento\"),\n",
        "        (\"Texas\", \"Austin\"),\n",
        "        (\"Florida\", \"Tallahassee\"),\n",
        "        (\"Illinois\", \"Springfield\"),\n",
        "        (\"Hawaii\", \"Honolulu\")\n",
        "    ],\n",
        "    \"positive_comperative\": [\n",
        "        (\"good\", \"better\"),\n",
        "        (\"nice\", \"nicer\"),\n",
        "        (\"tall\", \"taller\"),\n",
        "        (\"big\", \"bigger\"),\n",
        "        (\"dry\", \"drier\")\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "MC3EfOc3WKSp"
      },
      "id": "MC3EfOc3WKSp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For convenience I will extract words and its embeddings to long lists and stack embeddings for PCA."
      ],
      "metadata": {
        "id": "buXv2wz0WPpI"
      },
      "id": "buXv2wz0WPpI"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "embeddings = []\n",
        "words = []\n",
        "\n",
        "for class_name, pairs in semantic_classes.items():\n",
        "    for w1, w2 in pairs:\n",
        "        e1 = get_word_embedding(w1)\n",
        "        e2 = get_word_embedding(w2)\n",
        "        embeddings.append(e1.cpu().numpy())\n",
        "        words.append(w1)\n",
        "        embeddings.append(e2.cpu().numpy())\n",
        "        words.append(w2)\n",
        "X = np.vstack(embeddings)"
      ],
      "metadata": {
        "id": "KLOK-9HOW1Ow"
      },
      "id": "KLOK-9HOW1Ow",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to reduce dimensionality and visually inspect our embeddings."
      ],
      "metadata": {
        "id": "9VZ4JIArX3W6"
      },
      "id": "9VZ4JIArX3W6"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Perform PCA.\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Plot global PCA.\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.7)\n",
        "for i, word in enumerate(words):\n",
        "    x, y = X_pca[i]\n",
        "    plt.text(x + 0.02, y + 0.02, word, fontsize=8)\n",
        "\n",
        "plt.title(\"Global PCA\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AAiYF5NbX3lI"
      },
      "id": "AAiYF5NbX3lI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot gives some insight. Look at them bottom. Countries and cities are clumped together. On the right side we can see words from musicians and instruments class. Animals and meat lies on the top of the plot. On the left side there are adjectives close to human-denoting nouns."
      ],
      "metadata": {
        "id": "p-cxIsgiZDQS"
      },
      "id": "p-cxIsgiZDQS"
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot global PCA, but one class per image.\n",
        "classes_list = list(semantic_classes.keys())\n",
        "for j, class_name in enumerate(classes_list):\n",
        "    start = 10 * j\n",
        "    end = start + 10\n",
        "    coords = X_pca[start:end]\n",
        "    labels = words[start:end]\n",
        "\n",
        "    cmap = plt.colormaps.get_cmap('tab10')\n",
        "    pair_colors = [cmap(i) for i in range(5)]\n",
        "    colors = []\n",
        "    for pair_idx in range(5):\n",
        "        colors.append(pair_colors[pair_idx])\n",
        "        colors.append(pair_colors[pair_idx])\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.scatter(coords[:, 0], coords[:, 1], c=colors, alpha=0.7)\n",
        "    for k, word in enumerate(labels):\n",
        "        x, y = coords[k]\n",
        "        plt.text(x + 0.02, y + 0.02, word, fontsize=8)\n",
        "\n",
        "    plt.title(f\"Global PCA, class: {class_name}\")\n",
        "    plt.xlabel(\"PC1\")\n",
        "    plt.ylabel(\"PC2\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "k8-4oc4qZKyI"
      },
      "id": "k8-4oc4qZKyI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These separated charts shed additional light. Visually, you can confirm that each class's words remain roughly in the same subregion of the 2D space. Some classes form tighter clouds, others spread out more. Plot with countries and currencies is interesting. There you can clearly separate those two."
      ],
      "metadata": {
        "id": "R7f6C9Rfedoy"
      },
      "id": "R7f6C9Rfedoy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I will perform $10$ PCA's, one for each class."
      ],
      "metadata": {
        "id": "YYVre7gzf-7M"
      },
      "id": "YYVre7gzf-7M"
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot 10 PCA's, one for each class.\n",
        "for j, class_name in enumerate(classes_list):\n",
        "    start = 10 * j\n",
        "    end = start + 10\n",
        "    Xj = X_scaled[start:end]\n",
        "    labels_j = words[start:end]\n",
        "    pca_j  = PCA(n_components=2)\n",
        "    Xj_pca = pca_j.fit_transform(Xj)\n",
        "\n",
        "    cmap = plt.colormaps.get_cmap('tab10')\n",
        "    pair_colors = [cmap(i) for i in range(5)]\n",
        "    colors = []\n",
        "    for pair_idx in range(5):\n",
        "        colors.append(pair_colors[pair_idx])\n",
        "        colors.append(pair_colors[pair_idx])\n",
        "\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    plt.scatter(Xj_pca[:, 0], Xj_pca[:, 1], c=colors, alpha=0.7)\n",
        "    for k, word in enumerate(labels_j):\n",
        "        x, y = Xj_pca[k]\n",
        "        plt.text(x + 0.02, y + 0.02, word, fontsize=8)\n",
        "    plt.title(f\"Local PCA, class {class_name}\")\n",
        "    plt.xlabel(\"PC1\")\n",
        "    plt.ylabel(\"PC2\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "izP6LrplgNDm"
      },
      "id": "izP6LrplgNDm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the first and second figures, the words are beautifully separated, with each pair located very close together. In the third figure, the difference vectors between pairs are roughly the same length. The plot showing countries and currencies is also intriguing, notice that “euro” is positioned noticeably farther away from the other terms."
      ],
      "metadata": {
        "id": "Fo1OW8jmgkq8"
      },
      "id": "Fo1OW8jmgkq8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's examine arithmetic of embeddings."
      ],
      "metadata": {
        "id": "V0OmSVzfigrp"
      },
      "id": "V0OmSVzfigrp"
    },
    {
      "cell_type": "code",
      "source": [
        "# For each class calculate B-A+C, find 5 closest words, check if it is close to D, where (A,B), (C,D) are embeddings of two first pairs\n",
        "embedding_matrix = model.get_input_embeddings().weight.detach().cpu().numpy()\n",
        "\n",
        "def find_top_5_neighbors(S):\n",
        "    diffs = embedding_matrix - S\n",
        "    dists = np.linalg.norm(diffs, axis=1)\n",
        "    topk_indices = np.argsort(dists)[:5]\n",
        "    neighbors = []\n",
        "    for idx in topk_indices:\n",
        "        token_str = tokenizer.convert_ids_to_tokens([idx])[0]\n",
        "        neighbors.append((token_str, float(dists[idx])))\n",
        "    return neighbors\n",
        "\n",
        "for j, class_name in enumerate(classes_list):\n",
        "    start = 10 * j\n",
        "    A = embeddings[start]\n",
        "    B = embeddings[start+1]\n",
        "    C = embeddings[start+2]\n",
        "    S = B - A + C\n",
        "    top5 = find_top_5_neighbors(S)\n",
        "    print(f\"5 closest embeddings to {words[start+1]} - {words[start]} + {words[start+2]}:\")\n",
        "    for token, dist in top5:\n",
        "        print(f\"{token}  dist = {dist:.4f}\")\n"
      ],
      "metadata": {
        "id": "YGmJiag1ihP3"
      },
      "id": "YGmJiag1ihP3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In most cases, the expected word (D) appears among the top five nearest neighbors, although it is never the very closest. Some tokens that show up are unusual,for example, in the U.S. states and capitals class, three [unused] tokens appear. Those exist to support fine-tuning, allowing users to add new entries to the vocabulary. You'll also sometimes see numeric tokens or subword fragments like ##ess."
      ],
      "metadata": {
        "id": "NzR7yD9qjN5q"
      },
      "id": "NzR7yD9qjN5q"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}